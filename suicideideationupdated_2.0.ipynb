{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481cc290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "data = pd.read_csv('C:/Users/HP/Desktop/suiciderisk.csv', sep =',',engine = 'python')\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04a2a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summarize responses to create an overall risk response\n",
    "\n",
    "# assuming the risk response is the sum of all questions responses divided by the number of questions\n",
    "# Select only numeric columns before calculating mean\n",
    "numeric_data = data.select_dtypes(include='number')\n",
    "data['response'] = numeric_data.mean(axis=1).round().astype(int)\n",
    "\n",
    "# Define the mapping function\n",
    "def map_risk(response):\n",
    "    if response in [1, 2]:\n",
    "        return 'Low_risk'\n",
    "    elif response == 3:\n",
    "        return 'Medium_risk'\n",
    "    elif response in [4, 5]:\n",
    "        return 'High_risk'\n",
    "# Apply the mapping function\n",
    "data['risk_category'] = data['response'].apply(map_risk)\n",
    "\n",
    "# Features and labels\n",
    "X = data.drop(['response', 'risk_category'], axis=1)\n",
    "y = data['risk_category']\n",
    "\n",
    "# Display the Updated data\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbaed7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assume 'response' is the feature indicating the response of an individual\n",
    "# and 'risk_category' is the target variable we are trying to predict\n",
    "\n",
    "# Encode all categorical variables\n",
    "le = LabelEncoder()\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "\n",
    "# Split data into response (feature) and target (risk category)\n",
    "X = data.drop(['response','risk_category'], axis=1)  # Features\n",
    "y = data['risk_category']  # Target variable\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=500)\n",
    "}\n",
    "\n",
    "# Evaluation metric\n",
    "metrics = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc_ovr']\n",
    "\n",
    "# Evaluate models using k-fold cross-validation\n",
    "k = 5  # Number of folds\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    model_results = {}\n",
    "    for metric in metrics:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=k, scoring=metric)\n",
    "        model_results[metric] = cv_scores\n",
    "        results[model_name] = model_results\n",
    "        \n",
    "# Calculate the average score for each model\n",
    "average_scores = {}\n",
    "for model_name, scores in results.items():\n",
    "    average_score = sum([np.mean(score) for score in scores.values()]) / len(scores)\n",
    "    average_scores[model_name] = average_score\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = max(average_scores, key=average_scores.get)\n",
    "        \n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"  {metric_name}: {np.mean(score):.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac971fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "# 1. Compound bar chart of machine learning algorithms and the result of the evaluation metrics\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 16))\n",
    "fig.suptitle('Model Evaluation Metrics')\n",
    "\n",
    "# Prepare data for the compound bar chart\n",
    "metrics_means = {metric: [np.mean(results[model][metric]) for model in models] for metric in metrics}\n",
    "bar_width = 0.15\n",
    "index = np.arange(len(models))\n",
    "\n",
    "# Vertical bar chart\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax[0].bar(index + i * bar_width, metrics_means[metric], bar_width, label=metric)\n",
    "\n",
    "ax[0].set_xlabel('Model')\n",
    "ax[0].set_ylabel('Score')\n",
    "ax[0].set_title('Vertical Compound Bar Plot of Evaluation Metrics')\n",
    "ax[0].set_xticks(index + bar_width * (len(metrics) - 1) / 2)\n",
    "ax[0].set_xticklabels(models.keys())\n",
    "ax[0].set_ylim(0, 1)\n",
    "ax[0].legend()\n",
    "\n",
    "# Horizontal bar chart\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax[1].barh(index + i * bar_width, metrics_means[metric], bar_width, label=metric)\n",
    "\n",
    "ax[1].set_ylabel('Model')\n",
    "ax[1].set_xlabel('Score')\n",
    "ax[1].set_title('Horizontal Compound Bar Plot of Evaluation Metrics')\n",
    "ax[1].set_yticks(index + bar_width * (len(metrics) - 1) / 2)\n",
    "ax[1].set_yticklabels(models.keys())\n",
    "ax[1].set_xlim(0, 1)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# 2. Bar chart plot of individual evaluation metrics with the machine learning algorithms\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(10, len(metrics) * 5))\n",
    "fig.suptitle('Individual Evaluation Metrics by Model')\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    scores = [np.mean(results[model][metric]) for model in models]\n",
    "    axes[i].bar(models.keys(), scores)\n",
    "    axes[i].set_title(f'{metric.capitalize()}')\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all categorical variables\n",
    "le = LabelEncoder()\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop('risk_category', axis=1)  # Features\n",
    "y = data['risk_category']  # Target variable\n",
    "\n",
    "# Check the distribution of classes\n",
    "print(\"Class distribution in original data:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Hold out 10 examples for validation\n",
    "X_holdout, X_remaining, y_holdout, y_remaining = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "# Check the distribution of classes in the holdout set and the rest\n",
    "print(\"\\nClass distribution in holdout data:\")\n",
    "print(y_holdout.value_counts())\n",
    "print(\"\\nClass distribution in rest of the data:\")\n",
    "print(y_remaining.value_counts())\n",
    "\n",
    "\n",
    "# Split the remaining data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_remaining, y_remaining, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the target variable into a binary format\n",
    "le_target = LabelEncoder()\n",
    "y_train_binary = le_target.fit_transform(y_train)\n",
    "y_test_binary = le_target.transform(y_test)\n",
    "y_holdout_binary = le_target.transform(y_holdout)\n",
    "y_remaining_binary = le_target.transform(y_remaining)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=500)\n",
    "}\n",
    "\n",
    "# Evaluation metrics\n",
    "metrics = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc_ovr']\n",
    "\n",
    "# Evaluate models using k-fold cross-validation\n",
    "k = 5  # Number of folds\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    model_results = {}\n",
    "    for metric in metrics:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=k, scoring=metric)\n",
    "        model_results[metric] = cv_scores.mean()\n",
    "\n",
    "        results[model_name] = model_results\n",
    "\n",
    "\n",
    "# Calculate the average score for each model\n",
    "average_scores = {model_name: sum(scores.values()) / len(scores) for model_name, scores in results.items()}\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = max(average_scores, key=average_scores.get)\n",
    "\n",
    "# Train the best model on the entire remaining data\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X_remaining, y_remaining_binary)\n",
    "\n",
    "# Validate the best model on the held-out examples\n",
    "y_pred = best_model.predict(X_holdout)\n",
    "y_pred_proba = best_model.predict_proba(X_holdout)[:, 1]\n",
    "\n",
    "print(\"\\nValidation results:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_holdout_binary, y_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_holdout_binary, y_pred, average='weighted'):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_holdout_binary, y_pred, average='weighted'):.4f}\")\n",
    "print(f\"  F1-score: {f1_score(y_holdout_binary, y_pred, average='weighted'):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_holdout_binary, y_pred_proba):.4f}\")\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc194acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

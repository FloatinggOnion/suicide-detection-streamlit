{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481cc290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "data = pd.read_csv('C:/Users/HP/Desktop/suiciderisk.csv', sep =',',engine = 'python')\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04a2a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summarize responses to create an overall risk response\n",
    "\n",
    "# assuming the risk response is the sum of all questions responses divided by the number of questions\n",
    "# Select only numeric columns before calculating mean\n",
    "numeric_data = data.select_dtypes(include='number')\n",
    "data['response'] = numeric_data.mean(axis=1).round().astype(int)\n",
    "\n",
    "# Define the mapping function\n",
    "def map_risk(response):\n",
    "    if response in [1, 2]:\n",
    "        return 'Low_risk'\n",
    "    elif response == 3:\n",
    "        return 'Medium_risk'\n",
    "    elif response in [4, 5]:\n",
    "        return 'High_risk'\n",
    "# Apply the mapping function\n",
    "data['risk_category'] = data['response'].apply(map_risk)\n",
    "\n",
    "# Features and labels\n",
    "X = data.drop(['response', 'risk_category'], axis=1)\n",
    "y = data['risk_category']\n",
    "\n",
    "# Display the Updated data\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'response' is the feature indicating the response of an individual\n",
    "# and 'risk_category' is the target variable we are trying to predict\n",
    "\n",
    "# Encode all categorical variables\n",
    "le = LabelEncoder()\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "\n",
    "# Split data into response (feature) and target (risk category)\n",
    "X = data.drop('response', axis=1)  # Features\n",
    "y = data['risk_category']  # Target variable\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=500)\n",
    "}\n",
    "\n",
    "# Evaluation metrics\n",
    "metrics = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc_ovr']\n",
    "\n",
    "# Evaluate models using k-fold cross-validation\n",
    "k = 5  # Number of folds\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    model_results = {}\n",
    "    for metric in metrics:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=k, scoring=metric)\n",
    "        model_results[metric] = cv_scores.mean()\n",
    "\n",
    "        results[model_name] = model_results\n",
    "\n",
    "\n",
    "# Calculate the average score for each model\n",
    "average_scores = {model_name: sum(scores.values()) / len(scores) for model_name, scores in results.items()}\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = max(average_scores, key=average_scores.get)\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"\\nCross-validation results:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric_name, score in metrics.items():\n",
    "        print(f\"  {metric_name}: {score:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4057304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and their evaluation metrics\n",
    "models = ['Naive Bayes', 'SVM', 'Random Forest', 'Logistic Regression']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC']\n",
    "\n",
    "# Evaluation results\n",
    "results = {\n",
    "    'Naive Bayes': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 0.9556\n",
    "    },\n",
    "    'SVM': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 1.0000\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'Accuracy': 0.9975,\n",
    "        'Precision': 0.9975,\n",
    "        'Recall': 0.9975,\n",
    "        'F1-score': 0.9971,\n",
    "        'ROC AUC': 1.0000\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 1.0000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare data for plotting (vertical)\n",
    "x = np.arange(len(models))  # the label locations\n",
    "width = 0.10  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Plot each metric\n",
    "for i, metric in enumerate(metrics):\n",
    "    scores = [results[model][metric] for model in models]\n",
    "    ax.bar(x + i*width, scores, width, label=metric)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Machine Learning Models')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Evaluation Metrics for Suicide Ideation Prediction Models')\n",
    "ax.set_xticks(x + 2*width)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b506cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the models and their evaluation metrics\n",
    "models = ['Naive Bayes', 'SVM', 'Random Forest', 'Logistic Regression']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC']\n",
    "\n",
    "# Evaluation results \n",
    "results = {\n",
    "    'Naive Bayes': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 0.9556\n",
    "    },\n",
    "    'SVM': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 1.0000\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'Accuracy': 0.9975,\n",
    "        'Precision': 0.9975,\n",
    "        'Recall': 0.9975,\n",
    "        'F1-score': 0.9971,\n",
    "        'ROC AUC': 1.0000\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 1.0000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare data for plotting (horizontal)\n",
    "y_pos = np.arange(len(models))  # the label locations\n",
    "metrics_scores = {metric: [results[model][metric] for model in models] for metric in metrics}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "# Plot each metric\n",
    "bar_width = 0.15\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.barh(y_pos + i*bar_width, metrics_scores[metric], bar_width, label=metric)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Score')\n",
    "ax.set_ylabel('Machine Learning Models')\n",
    "ax.set_title('Evaluation Metrics for Suicide Ideation Prediction Models')\n",
    "ax.set_yticks(y_pos + 2*bar_width)\n",
    "ax.set_yticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the models and their evaluation metrics\n",
    "models = ['Naive Bayes', 'SVM', 'Random Forest', 'Logistic Regression']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC']\n",
    "\n",
    "# Evaluation results (replace with your actual results)\n",
    "results = {\n",
    "    'Naive Bayes': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 0.9556\n",
    "    },\n",
    "    'SVM': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 1.0000\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'Accuracy': 0.9975,\n",
    "        'Precision': 0.9975,\n",
    "        'Recall': 0.9975,\n",
    "        'F1-score': 0.9971,\n",
    "        'ROC AUC': 1.0000\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 1.0000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare data for plotting(individual vertical)\n",
    "metrics_scores = {metric: [results[model][metric] for model in models] for metric in metrics}\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(metrics), ncols=1, figsize=(7, 12))\n",
    "\n",
    "# Plot each metric\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axs[i]\n",
    "    ax.bar(models, [metrics_scores[metric][j] for j in range(len(models))], color=plt.cm.Paired(np.arange(len(models))))\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the models and their evaluation metrics\n",
    "models = ['Naive Bayes', 'SVM', 'Random Forest', 'Logistic Regression']\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC']\n",
    "\n",
    "# Evaluation results (replace with your actual results)\n",
    "results = {\n",
    "    'Naive Bayes': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 0.9556\n",
    "    },\n",
    "    'SVM': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 1.0000\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'Accuracy': 0.9975,\n",
    "        'Precision': 0.9975,\n",
    "        'Recall': 0.9975,\n",
    "        'F1-score': 0.9971,\n",
    "        'ROC AUC': 1.0000\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'Accuracy': 0.9950,\n",
    "        'Precision': 0.9951,\n",
    "        'Recall': 0.9950,\n",
    "        'F1-score': 0.9943,\n",
    "        'ROC AUC': 1.0000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare data for plotting(individual horizontal)\n",
    "metrics_scores = {metric: [results[model][metric] for model in models] for metric in metrics}\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(metrics), ncols=1, figsize=(5, 12))\n",
    "\n",
    "# Plot each metric\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axs[i]\n",
    "    ax.barh(models, [metrics_scores[metric][j] for j in range(len(models))], color=plt.cm.Paired(np.arange(len(models))))\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_xlim([0, 1.05])\n",
    "    ax.invert_yaxis()  # Invert y-axis to display the highest score at the top\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
